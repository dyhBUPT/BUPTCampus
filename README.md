# BUPTCampus
BUPTCampus is a video-based visible-infrared dataset 
with approximately pixel-level aligned tracklet pairs
and single-camera auxiliary samples.

![radar](assets/radar.png)

## Experiments

![ablation](assets/ablation.png)

![sota](assets/sota.png)

## Data Preparation

1\. Download BUPTCampus from baidu disk (TBD). The file structure should be:
```
path_to_dataset
|—— DATA
|—— data_paths.json
|—— gallery.txt
|—— query.txt
|—— train.txt
|—— train_auxiliary.txt
```
It contains all training/testing/auxiliary samples with 3,080 identities.
Moreover, in additional to the original RGB/IR samples,
the fake IR samples generated by our PairGAN module are also provided.

2\. Set the paths of your dataset to `--data_root` in `opt.py`.

Please note that (License):
- The dataset is only for academic. Please don't use it for commercial use.
- Please don't redistribute the dataset.
- Please cite our paper if you use the dataset.

By downloading our dataset, you agree to be bound by and comply with the license agreement.

## Requirements
- torch==1.11.0
- torchvision==0.12.0
- tensorboard==2.10.0
- numpy==1.23.1
- Pillow==7.1.2

## Test
For direct testing, please download our prepared checkpoints and extracted features from 
[baidu disk](https://pan.baidu.com/s/17yfHjKDhUevtfPLdgTMrNw?pwd=bupt).

#### 1) Baseline

Then run the following command to load the checkpoint, and you will get the results of baseline.
```shell script
python test.py --test_ckpt_path path/ckpt/ckpt_res34_real.pth
```

#### 2) AuxNet

To reproduce the reported performance of AuxNet, 
you can directly use the following command to perform re-ranking based on our extracted features.
```shell script
python re_ranking.py --test_feat_path path/feat
```

If you want to extract all these features by yourself, please use the following commands:
```shell script
python test.py --test_ckpt_path path/ckpt/ckpt_res34_real_auxiliary.pth --test_frame_sample uniform-first_half-second_half --feature_postfix _real-aux
python test.py --test_ckpt_path path/ckpt/ckpt_res34_fake_auxiliary.pth --test_frame_sample uniform-first_half-second_half --feature_postfix _fake-aux --fake
```
Then run `re_ranking.py`,  and you will get the final metrics of AuxNet.

## Train

#### 1) Baseline
You can train our baseline module by:
```shell script
python train.py --gpus 0,1
```

#### 2) AuxNet
To get the full AuxNet model, you should train the model twice.
First, you should train `real RGB` & `real IR` samples with auxiliary learning by:
```shell script
python train.py --gpus 0,1 --auxiliary
```
Then you will get the checkpoints corresponding to the provided `ckpt_res34_real_auxiliary.pth`

Second, you should train `fake IR` & `real IR` samples with auxiliary learning by:
```shell script
python train.py --gpus 0,1 --auxiliary --fake
```
Then you will get the checkpoints corresponding to the provided `ckpt_res34_fake_auxiliary.pth`

Finally, for evaluation, please refer to the `Test` section above (feature extraction + re_ranking).

## Citation
TBD

## Acknowledgement
A large part of codes are borrowed from 
[DDAG](https://github.com/mangye16/DDAG) and [FastReID](https://github.com/JDAI-CV/fast-reid).
Thanks for their excellent work!
